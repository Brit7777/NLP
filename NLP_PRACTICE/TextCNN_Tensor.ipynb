{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why CNN?\n",
    "**CNN** architecture reflects **the order of words / expressions in learning** by preserving local information in sentences\n",
    "\n",
    "**CNN**은 문장의 지역 정보를 보존함으로써 **단어/표현의 등장순서를 학습에 반영**하는 아키텍처\n",
    "> https://ratsgo.github.io/natural%20language%20processing/2017/03/19/CNN/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Practice Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf. reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN parameter\n",
    "embedding_size = 2 # bi-gram\n",
    "sequence_length = 3\n",
    "num_classes = 2 #0 or 1\n",
    "filter_sizes = [2,2,2]\n",
    "num_filters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'you', 'he', 'loves', 'me', 'she', 'likes', 'baseball', 'he', 'hates', 'football', 'i', 'hate', 'you', 'sorry', 'for', 'that', 'this', 'is', 'awful']\n",
      "['likes', 'me', 'sorry', 'that', 'awful', 'she', 'he', 'football', 'for', 'hates', 'is', 'you', 'i', 'baseball', 'hate', 'loves', 'love', 'this']\n",
      "{'likes': 0, 'me': 1, 'sorry': 2, 'that': 3, 'awful': 4, 'she': 5, 'he': 6, 'football': 7, 'for': 8, 'hates': 9, 'is': 10, 'you': 11, 'i': 12, 'baseball': 13, 'hate': 14, 'loves': 15, 'love': 16, 'this': 17}\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# 3 words sentences(sequence_length=3)\n",
    "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"he hates football\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "# present whether is the sentences is positive or negative\n",
    "labels = [1,1,1,0,0,0,0]\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "print(word_list)\n",
    "#remove duplicates\n",
    "word_list = list(set(word_list))\n",
    "print(word_list)\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "print(word_dict)\n",
    "vocab_size = len(word_dict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for sen in sentences : \n",
    "    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n",
    "\n",
    "outputs = []\n",
    "for out in labels:\n",
    "    outputs.append(np.eye(num_classes)[out]) # ONE-HOT : To using Tensor Softmax Loss function, eye함수는 단위행렬 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([12, 16, 11]), array([ 6, 15,  1]), array([ 5,  0, 13]), array([6, 9, 7]), array([12, 14, 11]), array([2, 8, 3]), array([17, 10,  4])]\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 1.]), array([0., 1.]), array([0., 1.]), array([1., 0.]), array([1., 0.]), array([1., 0.]), array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, num_classes])\n",
    "\n",
    "# make lookup table =  단어 ID들 각각을 벡터로 바꿔주기\n",
    "W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "embedded_chars = tf.nn.embedding_lookup(W, X) # [batch_size, sequence_length, embedding_size]\n",
    "embedded_chars = tf.expand_dims(embedded_chars, -1) # add channel(=1) [batch_size, sequence_length, embedding_size, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "    \n",
    "    # make conv layer\n",
    "    conv = tf.nn.conv2d(embedded_chars, # [batch_size, sequence_length, embedding_size, 1]\n",
    "                                    W,    # [filter_size, embedding_size, 1, num_filters(=3)]\n",
    "                                    strides=[1,1,1,1],  #배치데이터 하나씩, 단어 하나씩 슬라이딩하면서 보라는 의미\n",
    "                                    padding='VALID')\n",
    "    h= tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "    \n",
    "    # max-pooling\n",
    "    # ksize = Max-pooling하는 영역의 크기\n",
    "    pooled = tf.nn.max_pool(h,\n",
    "                            ksize=[1, sequence_length - filter_size + 1, 1, 1], # [batch_size, filter_height, filter_width, channel]\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='VALID')\n",
    "    pooled_outputs.append(pooled) # dim of pooled : [batch_size(=6), output_height(=1), output_width(=1), channel(=1)]\n",
    "    \n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs, num_filters) # h_pool : [batch_size(=6), output_height(=1), output_width(=1), channel(=1) * 3]\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total]) # [batch_size, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 000500 cost = 0.011613\n",
      "Epoch: 001000 cost = 0.001719\n",
      "Epoch: 001500 cost = 0.000649\n",
      "Epoch: 002000 cost = 0.000324\n",
      "Epoch: 002500 cost = 0.000185\n",
      "Epoch: 003000 cost = 0.000115\n",
      "Epoch: 003500 cost = 0.000075\n",
      "Epoch: 004000 cost = 0.000051\n",
      "Epoch: 004500 cost = 0.000035\n",
      "Epoch: 005000 cost = 0.000025\n",
      "sorry hate you contains negative meaning...\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "\n",
    "# Model-Training\n",
    "Weight = tf.get_variable('W', shape=[num_filters_total, num_classes], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "Bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "model = tf.nn.xw_plus_b(h_pool_flat, Weight, Bias)  \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# training\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(5000):\n",
    "    _, loss = sess.run([optimizer, cost], feed_dict={X: inputs, Y: outputs})\n",
    "    if (epoch+1)%500 == 0:\n",
    "        print('Epoch:', '%06d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "# Model-Predict\n",
    "hypothesis = tf.nn.softmax(model)\n",
    "predictions = tf.argmax(hypothesis, 1)\n",
    "\n",
    "# test\n",
    "test_text = 'sorry hate you'\n",
    "tests = []\n",
    "tests.append(np.asarray([word_dict[n] for n in test_text.split()]))\n",
    "\n",
    "predict = sess.run([predictions], feed_dict={X: tests})\n",
    "result = predict[0][0]\n",
    "if result == 0:\n",
    "    print(test_text,\"contains negative meaning...\")\n",
    "else:\n",
    "    print(test_text,\"contains positive meaning!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
