{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec vs FastText?\n",
    "- **Word2Vec** treats each word in corpus as an atomic entity and generates a vector for each word.\n",
    "- **FastText** treats each word as a component of character n-grams = sum of character n-gram\n",
    "\n",
    "    ex) “apple” = a sum of the vectors of the n-grams “<ap”, “app”, ”appl”, ”apple”, ”apple>”, “ppl”, “pple”, ”pple>”, “ple”, ”ple>”\n",
    "  \n",
    " *If you would like to know more, go through this [link](https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. pre-trained FastText\n",
    "They have pre-trained word vectors for 294 languages, trained on Wikipedia using FastText.\n",
    "> https://github.com/facebookresearch/fastText/blob/master/docs/pretrained-vectors.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#creating the model\n",
    "ko_model = KeyedVectors.load_word2vec_format('wiki.ko.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens : 879129\n",
      "Dimension of a word vector: 300\n",
      "vector components of a word : [-0.32184    0.54846   -1.0208    -0.048752  -0.22877   -0.029109\n",
      "  0.021888  -0.1674    -0.21267    0.22746   -0.02506    0.18203\n",
      " -0.019068   0.27718   -0.28098   -0.0038997  0.028645   0.34712\n",
      " -0.15245   -0.20516    0.19587   -0.012547   0.090326  -0.32227\n",
      " -0.068956   0.089779   0.12269    0.39683   -0.19468    0.73036\n",
      " -0.16096    0.033482  -0.13482   -0.26622   -0.27938   -0.33161\n",
      " -0.19468   -0.081854  -0.26334   -0.12199   -0.17464    0.21967\n",
      "  0.53648    0.042376   0.17449   -0.44221    0.16403   -0.10101\n",
      "  0.3519    -0.45333    0.24374    0.23487    0.069466  -0.2439\n",
      "  0.08941   -0.12421   -0.2672    -0.11752   -0.2772    -0.47857\n",
      "  0.023945   0.31361    0.29779   -0.32235    0.014144  -0.15962\n",
      " -0.12692    0.13397   -0.2002     0.094797   0.39331   -0.0046058\n",
      " -0.040946  -0.11802   -0.24577    0.20872   -0.13904    0.26253\n",
      " -0.46157    0.014196  -0.081839   0.38737    0.1862     0.089477\n",
      " -0.032213  -0.2122    -0.36256   -0.51599   -0.13213    0.69782\n",
      " -0.17655   -0.096388   0.39794   -0.09166   -0.01832   -0.06522\n",
      "  0.38676   -0.16377   -0.081462   0.47231    0.35644   -0.28807\n",
      " -0.35136   -0.27131    0.14167    0.26161    0.15922   -0.057018\n",
      " -0.032248  -0.24496    0.20533   -0.1522     0.11935    0.052642\n",
      "  0.15175   -0.33865    0.032439   0.35788   -0.25603    0.045699\n",
      "  0.33271    0.31732   -0.22548   -0.26673   -0.094014  -0.020172\n",
      "  0.39667    0.033908  -0.10558   -0.17606    0.26986    0.028813\n",
      " -0.23398    0.16692   -0.06601    0.12795    0.23217   -0.099224\n",
      "  0.28632   -0.2129    -0.10349   -0.018474   0.095209  -0.070554\n",
      "  0.065881  -0.13768    0.29558    0.10596   -0.53407    0.35923\n",
      " -0.19654    0.19683    0.13751    0.0052305 -0.045199   0.0014556\n",
      "  0.061526   0.17064    0.072417   0.31126    0.32778    0.51931\n",
      "  0.038994   0.23864    0.34613   -0.041563  -0.21482    0.060673\n",
      "  0.076613   0.096909   0.061293   0.18553    0.51975    0.44355\n",
      " -0.014183   0.51065    0.38834   -0.21573    0.21346    0.028\n",
      "  0.20287    0.046208  -0.28605    0.13225   -0.13229    0.67266\n",
      "  0.37871   -0.089946   0.38724   -0.58692   -0.20466    0.38873\n",
      "  0.24504    0.36115    0.04488   -0.0046594 -0.40368   -0.040361\n",
      " -0.1424    -0.25812    0.43007    0.37685   -0.13006    0.48665\n",
      " -0.33505   -0.3406    -0.36723   -0.32151   -0.49636   -0.40811\n",
      " -0.35011    0.28427    0.10231   -0.33352   -0.04995   -0.40929\n",
      " -0.38343    0.17607    0.073384   0.31754    0.0077188  0.052937\n",
      "  0.26615   -0.47127    0.50214    0.2425     0.38463   -0.12518\n",
      "  0.2651     0.036593  -0.27396    0.18222   -0.18041   -0.41217\n",
      " -0.3509     0.041531  -0.1891    -0.082541   0.39477    0.085722\n",
      "  0.36111   -0.13988   -0.24677    0.45214    0.12428   -0.43216\n",
      "  0.65293   -0.55347   -0.062718   0.16243   -0.22677   -0.46399\n",
      "  0.1427    -0.15906    0.012091   0.25926    0.02609    0.15886\n",
      "  0.72932    0.30763   -0.23541   -0.43117   -0.017772  -0.3811\n",
      "  0.26546   -0.22933    0.17986   -0.058121  -0.023434   0.30776\n",
      " -0.13809    0.23075    0.26484    0.27076    0.13577   -0.17161\n",
      " -0.11871    0.12891    0.27899    0.42391    0.083653   0.20848\n",
      " -0.49589   -0.042153  -0.40384   -0.22196    0.221     -0.074853\n",
      " -0.54906    0.28734    0.27864    0.44315   -0.51646    0.026518\n",
      "  0.24156   -0.019767  -0.11607   -0.32232   -0.029035  -0.089661 ]\n"
     ]
    }
   ],
   "source": [
    "#getting the tokens\n",
    "words = []\n",
    "for word in ko_model.vocab:\n",
    "    words.append(word)\n",
    "    \n",
    "#the total number of tokens\n",
    "print(\"# of tokens : {}\".format(len(words)))\n",
    "\n",
    "#the dimension of a word vector\n",
    "print(\"Dimension of a word vector: {}\".format(len(ko_model[word[0]])))\n",
    "\n",
    "#the vector of a word\n",
    "print(\"vector components of a word : {}\".format(ko_model[word[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'가'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      ".\n",
      ",\n",
      ")\n",
      "(\n",
      "년\n",
      "'\n",
      "-\n",
      "분류\n",
      "월\n",
      "일\n",
      "#\n",
      "}\n",
      "있다\n",
      "/\n",
      "~\n",
      "이\n",
      "《\n",
      "》\n",
      "는\n",
      "수\n",
      "제\n",
      "의\n",
      "넘겨주기\n",
      "은\n",
      "·\n",
      "있는\n",
      "그\n",
      "역\n",
      "kst\n",
      "대한민국의\n",
      "\\\n",
      "에\n",
      "토론\n",
      "선수\n",
      "바깥\n",
      "고리\n",
      "%\n",
      "한\n",
      "및\n",
      "를\n",
      "?\n",
      "축구\n",
      "한다\n",
      "the\n",
      "대한\n",
      "영화\n",
      "a\n",
      "을\n",
      "주\n"
     ]
    }
   ],
   "source": [
    "#examine the words in fasttext\n",
    "for word in words[:50]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 사랑사랑, Similarity: 0.81\n",
      "Word: 사랑치, Similarity: 0.78\n",
      "Word: 사랑일, Similarity: 0.77\n",
      "Word: 사랑느낌, Similarity: 0.76\n",
      "Word: 사랑이었네, Similarity: 0.76\n",
      "Word: 사랑이여, Similarity: 0.75\n",
      "Word: 사랑병, Similarity: 0.75\n",
      "Word: 사랑인, Similarity: 0.75\n",
      "Word: 사랑맛, Similarity: 0.75\n",
      "Word: 사랑노래, Similarity: 0.74\n"
     ]
    }
   ],
   "source": [
    "#pick your magic word\n",
    "find_similar_to = '사랑'\n",
    "\n",
    "#find similar word[default = top 10]\n",
    "for similar_word in ko_model.similar_by_word(find_similar_to):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(similar_word[0], similar_word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : 포유류, similarity : 0.72\n",
      "word : 포유동물, similarity : 0.71\n",
      "word : 절지동물, similarity : 0.69\n",
      "word : 양서류, similarity : 0.69\n",
      "word : 독동물, similarity : 0.69\n",
      "word : 포유류분류, similarity : 0.68\n",
      "word : 무척추동물, similarity : 0.68\n",
      "word : 척추동물분류, similarity : 0.68\n",
      "word : 도시동물, similarity : 0.68\n",
      "word : 동물상, similarity : 0.67\n"
     ]
    }
   ],
   "source": [
    "#test words1\n",
    "word_add = ['동물', '파충류']\n",
    "word_sub = ['뱀']\n",
    "\n",
    "#word vector calculation\n",
    "for result_word in ko_model.most_similar(positive=word_add, negative=word_sub):\n",
    "    print(\"word : {0}, similarity : {1:.2f}\".format(result_word[0],result_word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "된장국\n",
      "0.42482013\n",
      "0.6510483\n"
     ]
    }
   ],
   "source": [
    "#test words2\n",
    "word_match = ['아침', '점심', '저녁', '된장국']\n",
    "print(ko_model.doesnt_match(word_match))\n",
    "\n",
    "#test words3\n",
    "print(ko_model.similarity('컴퓨터', '인간'))\n",
    "\n",
    "#test words4\n",
    "print(ko_model.similarity('사랑해', '사랑합니다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Self-trained FastText by Gensim\n",
    "> https://radimrehurek.com/gensim/models/fasttext.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "ft_model = FastText(common_texts, size=4, window=3, min_count=1)\n",
    "#if your willing to train a model with your own data, use the code below\n",
    "#corpus_file = datapath('lee_background.cor')\n",
    "#ft_model.build_vocab(corpus_file=corpus_file)\n",
    "#total_words = ft_model.corpus_total_words\n",
    "#ft_model.train(corpus_file=cour, total_words=total_words, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('graph', 0.8333591818809509), ('system', 0.49392634630203247), ('user', 0.3329693675041199), ('trees', 0.22027269005775452), ('response', 0.007395058870315552), ('eps', -0.0833044946193695), ('survey', -0.11669595539569855), ('time', -0.17677493393421173), ('minors', -0.9261970520019531)]\n"
     ]
    }
   ],
   "source": [
    "common_texts[:100]\n",
    "\n",
    "similarities = ft_model.wv.most_similar(positive=['computer', 'human'], negative=['interface'])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec vs FastText?\n",
    "Most importantly, FastText can generate better word embeddings for rare words.\n",
    "Let's take a look with actual examples!\n",
    "\n",
    "In case of Word2Vec,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'electrofishing' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e2767ea518d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwv_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "wv_model = Word2Vec(common_texts, size=4, window=3, min_count=1)\n",
    "a=wv_model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since I don't have any proper vocabulary list, I will just comment the below.\n",
    "#ft_model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/35142536/60235471-d9770e80-98e2-11e9-8a60-f979fb185bbb.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
